import numpy as np
import pandas as pd
import json
import argparse
from pathlib import Path
from minio import Minio
from sklearn.model_selection import train_test_split, GridSearchCV, KFold, RandomizedSearchCV,cross_val_score,cross_val_predict
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from kfp import dsl
from io import StringIO
from kfp.components import load_component_from_file
import io
import plot as pl
import feature_selection as fs


def clean_split_data(args):
    
    #btap_data_df = get_data(args.tenant,args.bucket,args.obj_name)
    # skip row 1 so pandas can parse the data properly.
    
    with open(f'/vault/secrets/minio-{args.tenant}-tenant-1.json') as f:
        creds = json.load(f)
        minio_url = creds['MINIO_URL']

    import re
    # Get rid of http:// in minio URL
    http = re.compile('^https?://')

    # Create the minio client.
    client = Minio(
        http.sub("", creds['MINIO_URL']),
        access_key=creds['MINIO_ACCESS_KEY'],
        secret_key=creds['MINIO_SECRET_KEY'],
        secure=minio_url.startswith('https'),
        region="us-west-1"
    )
    
    #Reading data object from minio
    #data = client.get_object(bucket, obj_name)
    data = client.get_object(args.bucket, args.in_obj_name)
    data_decode = data.data.decode('utf-8')

    #converting data object to string
    data = StringIO(data_decode) 
    btap_data_df = pd.read_csv(data, skiprows=0, low_memory=False)
    half_count = len(btap_data_df) / 2
    btap_data_df = btap_data_df.dropna(thresh=half_count,axis=1) # Drop any column with more than 50% missing values
    
    
    # Data Cleaning, Dropping additional columns that could result in data leakeages
    """removing columns generated by the model which are not the target variables we are interested to predict
    - 'energy_eui_additional_fuel_gj_per_m_sq',
    - 'energy_eui_electricity_gj_per_m_sq'
    - 'energy_eui_natural_gas_gj_per_m_sq'
    """

    """
     'heating_peak_w_per_m_sq','cooling_peak_w_per_m_sq', 'bc_step_code_tedi_kwh_per_m_sq',	'bc_step_code_meui_kwh_per_m_sq'	
    
    """
    #print(btap_data_df.columns)
    drop_list= ['Unnamed: 0', ':datapoint_id',
       'heating_peak_w_per_m_sq','cooling_peak_w_per_m_sq', 'bc_step_code_tedi_kwh_per_m_sq','bc_step_code_meui_kwh_per_m_sq',
       'cost_utility_ghg_electricity_kg_per_m_sq',
       'cost_utility_ghg_natural gas_kg_per_m_sq',
       'cost_utility_ghg_total_kg_per_m_sq',
       'cost_utility_neb_electricity_cost_per_m_sq',
       'cost_utility_neb_natural gas_cost_per_m_sq',
       'cost_utility_neb_total_cost_per_m_sq','energy_eui_cooling_gj_per_m_sq', 
       'energy_eui_fans_gj_per_m_sq', 'energy_eui_heat recovery_gj_per_m_sq',
       'energy_eui_heating_gj_per_m_sq', 
       'energy_eui_pumps_gj_per_m_sq', 'energy_eui_total_gj_per_m_sq',
       'energy_eui_water systems_gj_per_m_sq',
       'energy_peak_electric_w_per_m_sq', 'energy_peak_natural_gas_w_per_m_sq',
       'net_site_eui_gj_per_m_sq', 'peak_cooling_load_w_per_m_sq_necb',
       'peak_heating_load_w_per_m_sq_necb',
       'phius_annual_cooling_demand_kwh_per_m_sq',
       'phius_annual_heating_demand_kwh_per_m_sq',
       'phius_peak_heating_load_w_per_m_sq', 'shw_natural_gas_per_year',
       'simulation_date', 'run_options', 'simulation_time',
       'total_site_eui_gj_per_m_sq', 'unmet_hours_cooling',
       'unmet_hours_cooling_during_occupied', 'datapoint_output_url',
       'bldg_fdwr', 'bldg_standards_template',
       'energy_eui_interior equipment_gj_per_m_sq',
       'energy_eui_interior lighting_gj_per_m_sq',
       'energy_principal_heating_source',
       'env_fdwr', 'env_ground_floors_average_conductance-w_per_m_sq_k',
       'env_outdoor_roofs_average_conductance-w_per_m_sq_k',
       'env_outdoor_walls_average_conductance-w_per_m_sq_k',
       'env_outdoor_windows_average_conductance-w_per_m_sq_k',
       'phius_necb_meet_cooling_demand', 'phius_necb_meet_heating_demand',
       'phius_peak_cooling_load_w_per_m_sq', 'shw_electricity_per_day',
       'shw_electricity_per_day_per_occupant', 'shw_electricity_per_year',
       'shw_total_nominal_occupancy', 'shw_water_m_cu_per_day',
       'shw_water_m_cu_per_day_per_occupant', 'shw_water_m_cu_per_year',
       'unmet_hours_heating', 'unmet_hours_heating_during_occupied']

    btap_data_df = btap_data_df.drop(drop_list,axis=1)
    
    for col in btap_data_df.columns:
        if len(btap_data_df[col].unique()) ==1:
            btap_data_df.drop(col,inplace=True,axis=1)
        
    #removing redundant variables
    drop_list = [':erv_package',  ':template']
    btap_data_df = btap_data_df.drop(drop_list,axis=1)

    #removing redundant variables not related to energy and with no description

    drop_list = [':chiller_type', ':oa_scale',':infiltration_scale',
             ':adv_dx_units',':pv_ground_type',':rotation_degrees',':ground_floor_cond', ':daylighting_type']
    btap_data_df = btap_data_df.drop(drop_list,axis=1)
    
    
    #Again, there may be some columns with more than one unique value, but one value that has
    # insignificant frequency in the data set. Letâ€™s find and drop any columns with unique values
    # that appear fewer than four times:
    for col in btap_data_df.columns:
        num = len(btap_data_df[col].unique())
    
        if (len(btap_data_df[col].unique()) <4):
            btap_data_df.drop(col,inplace=True,axis=1)
    
    
    # Calculating the total energy consumed
    btap_data_df['Total Energy'] = pd.DataFrame(btap_data_df,columns=['energy_eui_additional_fuel_gj_per_m_sq','energy_eui_electricity_gj_per_m_sq','energy_eui_natural_gas_gj_per_m_sq'],).sum(axis=1)
    drop_list=['energy_eui_additional_fuel_gj_per_m_sq','energy_eui_electricity_gj_per_m_sq','energy_eui_natural_gas_gj_per_m_sq']
    btap_data_df = btap_data_df.drop(drop_list,axis=1)
    
    #split to train and test datasets
    y = btap_data_df['Total Energy']
    X = btap_data_df.drop(['Total Energy'],axis = 1)
    
    #saving the plots
    pl.show_var(btap_data_df)
    pl.corr_plot(btap_data_df)
    pl.norm_res_plot(btap_data_df)
    
    
    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 1)
    
    #standardize
    from sklearn.preprocessing import StandardScaler
    X_sc_train = StandardScaler(with_mean=0, with_std=1).fit_transform(X_train)
    X_sc_test = StandardScaler(with_mean=0, with_std=1).fit_transform(X_test)
   

    
    # Creates `data` structure to save and 
    # share train and test datasets.
    data = {'X_train' : X_sc_train.tolist(),
            'y_train' : y_train.values.tolist(),
            'X_test' : X_sc_test.tolist(),
            'y_test' : y_test.values.tolist()}
    
    # Creates a json object based on `data`
    data_json = json.dumps(data).encode('utf-8')
    csv_buffer = io.BytesIO(data_json)
    
    # Component that takes a file and puts it into minio
    copy_to_minio_op = load_component_from_file('./components/copy_to_minio.yaml')

#     #copy data to minio
    copy_sample = copy_to_minio_op(
            creds['MINIO_URL'],
            creds['MINIO_ACCESS_KEY'],
            creds['MINIO_SECRET_KEY'],
            "this is a test",
            "oluwabukola-ishola/btap_data/new_data",
            )
   #     #upload dataframe to minio
    #df = pd.concat([X_train, X_test,Y_test,Y_train], axis =1)
    #csv_bytes = df.to_csv().encode('utf-8')
    
#     
#     client.put_object(bucket_name=args.bucket,
#                   object_name="split_data_out",  
#                   data=csv_buffer, 
#                   length=len(data_json), 
#                   content_type='application/csv')
    


if __name__ == '__main__':
    
    # This component does not receive any input it only outpus one artifact which is `data`.
    # Defining and parsing the command-line arguments
    parser = argparse.ArgumentParser()
    
     # Paths must be passed in, not hardcoded
    parser.add_argument('--tenant', type=str, help='The minio tenant where the data is located in')
    parser.add_argument('--bucket', type=str, help='The minio bucket where the data is located in')
    parser.add_argument('--in_obj_name', type=str, help='Name of data file to be read')
    parser.add_argument('--output_path', type=str, help='Path of the local file where the output file should be written.')
    args = parser.parse_args()
    
    # Creating the directory where the output file will be created 
    # (the directory may or may not exist).
    Path(args.output_path).parent.mkdir(parents=True, exist_ok=True)
    clean_split_data(args)
    
    #to run the program use the command below
    #python3 preprocessing.py --tenant standard --bucket oluwabukola-ishola --obj_name btap_data.csv  --output_path ./btap-data
